{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 逻辑回归\n",
    "\n",
    "softmax with logits with cross entropy，二分类就是logistic regression，多分类就是softmax regression。\n",
    "\n",
    "释义\n",
    "* logits - 是softmax的输入，即不做sigmoid只做sum的输出值\n",
    "* softmax - 是简单的计算方式\n",
    "* cross entropy - 是训练所使用的cost函数\n",
    "\n",
    "二分类问题的cost函数是最小二乘的极大似然估计，推广到多分类上就是cross entropy交叉熵，熵、互信息这些概念在后面的无监督学习也常用到。比如VAE中提到的kl divergence，KL散度\n",
    "\n",
    "二分类的极大似然估计 $L(\\theta) = y*log(h_{\\theta}(X)) + (1-y) * log(1 - log(h_{\\theta}(X)) $\n",
    "\n",
    "多分类的交叉熵 $-\\sum p(x)logp(x)$ => $ -\\sum y_i * log(h_{\\theta}(X)) $ 可见$-L(\\theta)$就是二分类的交叉熵了。\n",
    "\n",
    "下面先用pytorch来实现一个简单的逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# read mnist\n",
    "trainLoader = torch.utils.data.DataLoader(datasets.MNIST('./data', train = True, download = True, \n",
    "                                                         transform = transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Normalize((0.1307,), (0.3081,)), # ????? \n",
    "                                                             # I think those are the mean and std deviation of the MNIST dataset.\n",
    "                                                         ])),\n",
    "                                         batch_size = 32, shuffle = True)\n",
    "testLoader = torch.utils.data.DataLoader(datasets.MNIST('./data', train = False, download = True,\n",
    "                                                       transform = transforms.Compose([\n",
    "                                                           transforms.ToTensor(),\n",
    "                                                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                                       ])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457\n",
    "I think those are the mean and std deviation of the MNIST dataset. 这句话的出处， 伟大的avijit_dasgupta\n",
    "但问题是，为什么pytorch要这么做，要用户自己去计算数据集的mean和std？？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def buildLRModel():\n",
    "    model = nn.Sequential()\n",
    "    model.add_module(\"simple\", nn.Linear(784, 10, bias = True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意哦，这里的输入可以二维的图像，而我们的输入希望是一个全连接，所以需要加一个view，这个名字，给pytorch100分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function view in module torch.autograd.variable:\n",
      "\n",
      "view(self, *sizes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Variable.view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.49, Accuracy: 3.12\n",
      "loss: 0.58, Accuracy: 81.25\n",
      "loss: 0.47, Accuracy: 84.38\n",
      "loss: 0.65, Accuracy: 81.25\n",
      "loss: 0.41, Accuracy: 78.12\n",
      "loss: 0.34, Accuracy: 90.62\n",
      "loss: 0.27, Accuracy: 90.62\n",
      "loss: 0.37, Accuracy: 90.62\n",
      "loss: 0.18, Accuracy: 96.88\n",
      "loss: 0.15, Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "it = iter(trainLoader)\n",
    "model = buildLRModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    imgs, labels = next(it)\n",
    "    imgs, labels = Variable(imgs), Variable(labels)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    x = imgs.view(imgs.size(0), 784)\n",
    "    y = model.forward(x)\n",
    "    loss = criterion(y, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        _, predicted = torch.max(y, 1)\n",
    "        total = labels.size(0)\n",
    "        correct = (predicted == labels).sum()\n",
    "        print(\"loss: %.2f, Accuracy: %.2f\" % (loss.data[0], correct.data[0] / total * 100 ))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上Sequential是一个比较麻烦的东西，显然pytorch通过制造这个恶心玩意儿，比我们只能使用类，继承Module，好吧，这么个简单模型我么也得这么干，重来一遍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LRModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LRModel, self).__init__()\n",
    "        self.fc = nn.Linear(784, 10, bias = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.60, Accuracy: 6.25\n",
      "loss: 0.66, Accuracy: 84.38\n",
      "loss: 0.48, Accuracy: 84.38\n",
      "loss: 0.42, Accuracy: 84.38\n",
      "loss: 0.31, Accuracy: 90.62\n",
      "loss: 0.41, Accuracy: 87.50\n",
      "loss: 0.34, Accuracy: 90.62\n",
      "loss: 0.40, Accuracy: 87.50\n",
      "loss: 0.48, Accuracy: 84.38\n",
      "loss: 0.35, Accuracy: 93.75\n"
     ]
    }
   ],
   "source": [
    "it = iter(trainLoader)\n",
    "model = LRModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    imgs, labels = next(it)\n",
    "    imgs, labels = Variable(imgs), Variable(labels)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    x = imgs.view(imgs.size(0), 784)\n",
    "    y = model(x)\n",
    "    loss = criterion(y, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        _, predicted = torch.max(y, 1)\n",
    "        total = labels.size(0)\n",
    "        correct = (predicted == labels).sum()\n",
    "        print(\"loss: %.2f, Accuracy: %.2f\" % (loss.data[0], correct.data[0] / total * 100 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
