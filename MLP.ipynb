{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "多层感知器模型，采用三层网络，ReLU，最后softmax，进行MNist的softmax回归，代码和lr比较相近，只是我们需要大建的Model不一样而已，所以我们会意识到，其实很多的模型，在训练层面大致的代码都差不多，未来，我们会这些统一的东西，进行封装，目前，为了加深印象，我们依然，要自己手写，直到能够熟练地背诵下来。熟能生巧嘛！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " 马上那个二货normalize又出来了，很恶心，我们先看看到底是个什么玩意儿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MNIST in module torchvision.datasets.mnist object:\n",
      "\n",
      "class MNIST(torch.utils.data.dataset.Dataset)\n",
      " |  `MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
      " |  \n",
      " |  Args:\n",
      " |      root (string): Root directory of dataset where ``processed/training.pt``\n",
      " |          and  ``processed/test.pt`` exist.\n",
      " |      train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      " |          otherwise from ``test.pt``.\n",
      " |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      " |          puts it in root directory. If dataset is already downloaded, it is not\n",
      " |          downloaded again.\n",
      " |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      " |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      " |      target_transform (callable, optional): A function/transform that takes in the\n",
      " |          target and transforms it.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MNIST\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Args:\n",
      " |          index (int): Index\n",
      " |      \n",
      " |      Returns:\n",
      " |          tuple: (image, target) where target is index of the target class.\n",
      " |  \n",
      " |  __init__(self, root, train=True, transform=None, target_transform=None, download=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  download(self)\n",
      " |      Download the MNIST data if it doesn't exist in processed_folder already.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  processed_folder = 'processed'\n",
      " |  \n",
      " |  raw_folder = 'raw'\n",
      " |  \n",
      " |  test_file = 'test.pt'\n",
      " |  \n",
      " |  training_file = 'training.pt'\n",
      " |  \n",
      " |  urls = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "Help on class Dataset in module torch.utils.data.dataset:\n",
      "\n",
      "class Dataset(builtins.object)\n",
      " |  An abstract class representing a Dataset.\n",
      " |  \n",
      " |  All other datasets should subclass it. All subclasses should override\n",
      " |  ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
      " |  supporting integer indexing in range from 0 to len(self) exclusive.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "<class 'torch.ByteTensor'>\n",
      "torch.Size([60000, 28, 28])\n",
      "Help on class ByteTensor in module torch:\n",
      "\n",
      "class ByteTensor(torch._C.ByteTensorBase, torch.tensor._TensorBase)\n",
      " |  Method resolution order:\n",
      " |      ByteTensor\n",
      " |      torch._C.ByteTensorBase\n",
      " |      torch.tensor._TensorBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  is_signed(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  storage_type() from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch._C.ByteTensorBase:\n",
      " |  \n",
      " |  __and__(...)\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __iand__(...)\n",
      " |  \n",
      " |  __ilshift__(...)\n",
      " |  \n",
      " |  __ior__(...)\n",
      " |  \n",
      " |  __irshift__(...)\n",
      " |  \n",
      " |  __ixor__(...)\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lshift__(...)\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __or__(...)\n",
      " |  \n",
      " |  __rshift__(...)\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __xor__(...)\n",
      " |  \n",
      " |  add(...)\n",
      " |      add(value)\n",
      " |      \n",
      " |      See :func:`torch.add`\n",
      " |  \n",
      " |  add_(...)\n",
      " |      add_(value)\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.add`\n",
      " |  \n",
      " |  addbmm(...)\n",
      " |      addbmm(beta=1, mat, alpha=1, batch1, batch2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addbmm`\n",
      " |  \n",
      " |  addbmm_(...)\n",
      " |      addbmm_(beta=1, mat, alpha=1, batch1, batch2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addbmm`\n",
      " |  \n",
      " |  addcdiv(...)\n",
      " |      addcdiv(value=1, tensor1, tensor2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addcdiv`\n",
      " |  \n",
      " |  addcdiv_(...)\n",
      " |      addcdiv_(value=1, tensor1, tensor2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addcdiv`\n",
      " |  \n",
      " |  addcmul(...)\n",
      " |      addcmul(value=1, tensor1, tensor2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addcmul`\n",
      " |  \n",
      " |  addcmul_(...)\n",
      " |      addcmul_(value=1, tensor1, tensor2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addcmul`\n",
      " |  \n",
      " |  addmm(...)\n",
      " |      addmm(beta=1, mat, alpha=1, mat1, mat2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addmm`\n",
      " |  \n",
      " |  addmm_(...)\n",
      " |      addmm_(beta=1, mat, alpha=1, mat1, mat2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addmm`\n",
      " |  \n",
      " |  addmv(...)\n",
      " |      addmv(beta=1, tensor, alpha=1, mat, vec) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addmv`\n",
      " |  \n",
      " |  addmv_(...)\n",
      " |      addmv_(beta=1, tensor, alpha=1, mat, vec) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addmv`\n",
      " |  \n",
      " |  addr(...)\n",
      " |      addr(beta=1, alpha=1, vec1, vec2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addr`\n",
      " |  \n",
      " |  addr_(...)\n",
      " |      addr_(beta=1, alpha=1, vec1, vec2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addr`\n",
      " |  \n",
      " |  all(...)\n",
      " |  \n",
      " |  any(...)\n",
      " |  \n",
      " |  apply_(...)\n",
      " |      apply_(callable) -> Tensor\n",
      " |      \n",
      " |      Applies the function :attr:`callable` to each element in the tensor, replacing\n",
      " |      each element with the value returned by :attr:`callable`.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          This function only works with CPU tensors and should not be used in code\n",
      " |          sections that require high performance.\n",
      " |  \n",
      " |  baddbmm(...)\n",
      " |      baddbmm(beta=1, alpha=1, batch1, batch2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.baddbmm`\n",
      " |  \n",
      " |  baddbmm_(...)\n",
      " |      baddbmm_(beta=1, alpha=1, batch1, batch2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.baddbmm`\n",
      " |  \n",
      " |  bernoulli_(...)\n",
      " |      bernoulli_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.bernoulli`\n",
      " |  \n",
      " |  bmm(...)\n",
      " |      bmm(batch2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.bmm`\n",
      " |  \n",
      " |  clamp(...)\n",
      " |      clamp(min, max) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.clamp`\n",
      " |  \n",
      " |  clamp_(...)\n",
      " |      clamp_(min, max) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.clamp`\n",
      " |  \n",
      " |  clone(...)\n",
      " |      clone() -> Tensor\n",
      " |      \n",
      " |      Returns a copy of the tensor. The copy has the same size and data type as the\n",
      " |      original tensor.\n",
      " |  \n",
      " |  contiguous(...)\n",
      " |      contiguous() -> Tensor\n",
      " |      \n",
      " |      Returns a contiguous Tensor containing the same data as this tensor. If this\n",
      " |      tensor is contiguous, this function returns the original tensor.\n",
      " |  \n",
      " |  copy_(...)\n",
      " |      copy_(src, async=False, broadcast=True) -> Tensor\n",
      " |      \n",
      " |      Copies the elements from :attr:`src` into this tensor and returns this tensor.\n",
      " |      \n",
      " |      If :attr:`broadcast` is True, the source tensor must be\n",
      " |      :ref:`broadcastable <broadcasting-semantics>` with this tensor. Otherwise,\n",
      " |      source tensor should have the same number of elements as this tensor.\n",
      " |      It may be of a different data type or reside on a different device.\n",
      " |      \n",
      " |      Args:\n",
      " |          src (Tensor): Source tensor to copy\n",
      " |          async (bool): If True and this copy is between CPU and GPU, then the copy\n",
      " |              may occur asynchronously with respect to the host. For other\n",
      " |              copies, this argument has no effect.\n",
      " |          broadcast (bool): If True, :attr:`src` will be broadcast to the shape of\n",
      " |              the underlying tensor.\n",
      " |  \n",
      " |  cross(...)\n",
      " |      cross(other, dim=-1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cross`\n",
      " |  \n",
      " |  cumprod(...)\n",
      " |      cumprod(dim) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cumprod`\n",
      " |  \n",
      " |  cumsum(...)\n",
      " |      cumsum(dim) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cumsum`\n",
      " |  \n",
      " |  data_ptr(...)\n",
      " |      data_ptr() -> int\n",
      " |      \n",
      " |      Returns the address of the first element of this tensor.\n",
      " |  \n",
      " |  diag(...)\n",
      " |      diag(diagonal=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.diag`\n",
      " |  \n",
      " |  dim(...)\n",
      " |      dim() -> int\n",
      " |      \n",
      " |      Returns the number of dimensions of this tensor.\n",
      " |  \n",
      " |  div(...)\n",
      " |      div(value)\n",
      " |      \n",
      " |      See :func:`torch.div`\n",
      " |  \n",
      " |  div_(...)\n",
      " |      div_(value)\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.div`\n",
      " |  \n",
      " |  dot(...)\n",
      " |      dot(tensor2) -> float\n",
      " |      \n",
      " |      See :func:`torch.dot`\n",
      " |  \n",
      " |  element_size(...)\n",
      " |      element_size() -> int\n",
      " |      \n",
      " |      Returns the size in bytes of an individual element.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> torch.FloatTensor().element_size()\n",
      " |          4\n",
      " |          >>> torch.ByteTensor().element_size()\n",
      " |          1\n",
      " |  \n",
      " |  eq(...)\n",
      " |      eq(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.eq`\n",
      " |  \n",
      " |  eq_(...)\n",
      " |      eq_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.eq`\n",
      " |  \n",
      " |  equal(...)\n",
      " |      equal(other) -> bool\n",
      " |      \n",
      " |      See :func:`torch.equal`\n",
      " |  \n",
      " |  expand(...)\n",
      " |      expand(tensor, sizes) -> Tensor\n",
      " |      \n",
      " |      Returns a new view of the tensor with singleton dimensions expanded\n",
      " |      to a larger size.\n",
      " |      \n",
      " |      Tensor can be also expanded to a larger number of dimensions, and the\n",
      " |      new ones will be appended at the front.\n",
      " |      \n",
      " |      Expanding a tensor does not allocate new memory, but only creates a\n",
      " |      new view on the existing tensor where a dimension of size one is\n",
      " |      expanded to a larger size by setting the ``stride`` to 0. Any dimension\n",
      " |      of size 1 can be expanded to an arbitrary value without allocating new\n",
      " |      memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          *sizes (torch.Size or int...): The desired expanded size\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> x = torch.Tensor([[1], [2], [3]])\n",
      " |          >>> x.size()\n",
      " |          torch.Size([3, 1])\n",
      " |          >>> x.expand(3, 4)\n",
      " |           1  1  1  1\n",
      " |           2  2  2  2\n",
      " |           3  3  3  3\n",
      " |          [torch.FloatTensor of size 3x4]\n",
      " |  \n",
      " |  fill_(...)\n",
      " |      fill_(value) -> Tensor\n",
      " |      \n",
      " |      Fills this tensor with the specified value.\n",
      " |  \n",
      " |  fmod(...)\n",
      " |      fmod(divisor) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.fmod`\n",
      " |  \n",
      " |  fmod_(...)\n",
      " |      fmod_(divisor) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.fmod`\n",
      " |  \n",
      " |  gather(...)\n",
      " |      gather(dim, index) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.gather`\n",
      " |  \n",
      " |  ge(...)\n",
      " |      ge(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ge`\n",
      " |  \n",
      " |  ge_(...)\n",
      " |      ge_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.ge`\n",
      " |  \n",
      " |  geometric_(...)\n",
      " |      geometric_(p, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills this tensor with elements drawn from the geometric distribution:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          P(X=k) = (1 - p)^{k - 1} p\n",
      " |  \n",
      " |  ger(...)\n",
      " |      ger(vec2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ger`\n",
      " |  \n",
      " |  gt(...)\n",
      " |      gt(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.gt`\n",
      " |  \n",
      " |  gt_(...)\n",
      " |      gt_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.gt`\n",
      " |  \n",
      " |  index(...)\n",
      " |      index(m) -> Tensor\n",
      " |      \n",
      " |      Selects elements from this tensor using a binary mask or along a given\n",
      " |      dimension. The expression ``tensor.index(m)`` is equivalent to ``tensor[m]``.\n",
      " |      \n",
      " |      Args:\n",
      " |          m (int or ByteTensor or slice): The dimension or mask used to select elements\n",
      " |  \n",
      " |  index_add_(...)\n",
      " |      index_add_(dim, index, tensor) -> Tensor\n",
      " |      \n",
      " |      Accumulate the elements of tensor into the original tensor by adding to the\n",
      " |      indices in the order given in index. The shape of tensor must exactly match the\n",
      " |      elements indexed or an error will be raised.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): Dimension along which to index\n",
      " |          index (LongTensor): Indices to select from tensor\n",
      " |          tensor (Tensor): Tensor containing values to add\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> x = torch.Tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
      " |          >>> t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      " |          >>> index = torch.LongTensor([0, 2, 1])\n",
      " |          >>> x.index_add_(0, index, t)\n",
      " |          >>> x\n",
      " |            2   3   4\n",
      " |            8   9  10\n",
      " |            5   6   7\n",
      " |          [torch.FloatTensor of size 3x3]\n",
      " |  \n",
      " |  index_copy_(...)\n",
      " |      index_copy_(dim, index, tensor) -> Tensor\n",
      " |      \n",
      " |      Copies the elements of tensor into the original tensor by selecting the\n",
      " |      indices in the order given in index. The shape of tensor must exactly match the\n",
      " |      elements indexed or an error will be raised.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): Dimension along which to index\n",
      " |          index (LongTensor): Indices to select from tensor\n",
      " |          tensor (Tensor): Tensor containing values to copy\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> x = torch.Tensor(3, 3)\n",
      " |          >>> t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      " |          >>> index = torch.LongTensor([0, 2, 1])\n",
      " |          >>> x.index_copy_(0, index, t)\n",
      " |          >>> x\n",
      " |           1  2  3\n",
      " |           7  8  9\n",
      " |           4  5  6\n",
      " |          [torch.FloatTensor of size 3x3]\n",
      " |  \n",
      " |  index_fill_(...)\n",
      " |      index_fill_(dim, index, val) -> Tensor\n",
      " |      \n",
      " |      Fills the elements of the original tensor with value :attr:`val` by selecting\n",
      " |      the indices in the order given in index.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): Dimension along which to index\n",
      " |          index (LongTensor): Indices\n",
      " |          val (float): Value to fill\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      " |          >>> index = torch.LongTensor([0, 2])\n",
      " |          >>> x.index_fill_(1, index, -1)\n",
      " |          >>> x\n",
      " |          -1  2 -1\n",
      " |          -1  5 -1\n",
      " |          -1  8 -1\n",
      " |          [torch.FloatTensor of size 3x3]\n",
      " |  \n",
      " |  index_select(...)\n",
      " |      index_select(dim, index) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.index_select`\n",
      " |  \n",
      " |  is_contiguous(...)\n",
      " |      is_contiguous() -> bool\n",
      " |      \n",
      " |      Returns True if this tensor is contiguous in memory in C order.\n",
      " |  \n",
      " |  is_same_size(...)\n",
      " |  \n",
      " |  is_set_to(...)\n",
      " |      is_set_to(tensor) -> bool\n",
      " |      \n",
      " |      Returns True if this object refers to the same ``THTensor`` object from the\n",
      " |      Torch C API as the given tensor.\n",
      " |  \n",
      " |  kthvalue(...)\n",
      " |      kthvalue(k, dim=None) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.kthvalue`\n",
      " |  \n",
      " |  le(...)\n",
      " |      le(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.le`\n",
      " |  \n",
      " |  le_(...)\n",
      " |      le_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.le`\n",
      " |  \n",
      " |  lt(...)\n",
      " |      lt(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.lt`\n",
      " |  \n",
      " |  lt_(...)\n",
      " |      lt_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.lt`\n",
      " |  \n",
      " |  map2_(...)\n",
      " |  \n",
      " |  map_(...)\n",
      " |      map_(tensor, callable)\n",
      " |      \n",
      " |      Applies :attr:`callable` for each element in this tensor and the given tensor\n",
      " |      and stores the results in this tensor.  This tensor and the given tensor must be\n",
      " |      :ref:`broadcastable <broadcasting-semantics>`.\n",
      " |      \n",
      " |      The :attr:`callable` should have the signature::\n",
      " |      \n",
      " |          def callable(a, b) -> number\n",
      " |  \n",
      " |  masked_fill_(...)\n",
      " |      masked_fill_(mask, value)\n",
      " |      \n",
      " |      Fills elements of this tensor with :attr:`value` where :attr:`mask` is one.\n",
      " |      The shape of :attr:`mask` must be :ref:`broadcastable <broadcasting-semantics>`\n",
      " |      with the shape of the underlying tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          mask (ByteTensor): The binary mask\n",
      " |          value (float): The value to fill\n",
      " |  \n",
      " |  masked_scatter_(...)\n",
      " |      masked_scatter_(mask, source)\n",
      " |      \n",
      " |      Copies elements from :attr:`source` into this tensor at positions where the\n",
      " |      :attr:`mask` is one.\n",
      " |      The shape of :attr:`mask` must be :ref:`broadcastable <broadcasting-semantics>`\n",
      " |      with the shape of the underlying tensor. The :attr:`source` should have at least\n",
      " |      as many elements as the number of ones in :attr:`mask`\n",
      " |      \n",
      " |      Args:\n",
      " |          mask (ByteTensor): The binary mask\n",
      " |          source (Tensor): The tensor to copy from\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          The :attr:`mask` operates on the :attr:`self` tensor, not on the given\n",
      " |          :attr:`source` tensor.\n",
      " |  \n",
      " |  masked_select(...)\n",
      " |      masked_select(mask) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.masked_select`\n",
      " |  \n",
      " |  max(...)\n",
      " |      max(dim=None) -> float or (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.max`\n",
      " |  \n",
      " |  median(...)\n",
      " |      median(dim=-1, values=None, indices=None) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.median`\n",
      " |  \n",
      " |  min(...)\n",
      " |      min(dim=None) -> float or (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.min`\n",
      " |  \n",
      " |  mm(...)\n",
      " |      mm(mat2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.mm`\n",
      " |  \n",
      " |  mode(...)\n",
      " |      mode(dim=-1, values=None, indices=None) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.mode`\n",
      " |  \n",
      " |  mul(...)\n",
      " |      mul(value) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.mul`\n",
      " |  \n",
      " |  mul_(...)\n",
      " |      mul_(value)\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.mul`\n",
      " |  \n",
      " |  mv(...)\n",
      " |      mv(vec) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.mv`\n",
      " |  \n",
      " |  narrow(...)\n",
      " |      narrow(dimension, start, length) -> Tensor\n",
      " |      \n",
      " |      Returns a new tensor that is a narrowed version of this tensor. The dimension\n",
      " |      :attr:`dim` is narrowed from :attr:`start` to :attr:`start + length`. The\n",
      " |      returned tensor and this tensor share the same underlying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          dimension (int): The dimension along which to narrow\n",
      " |          start (int): The starting dimension\n",
      " |          length (int):\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      " |          >>> x.narrow(0, 0, 2)\n",
      " |           1  2  3\n",
      " |           4  5  6\n",
      " |          [torch.FloatTensor of size 2x3]\n",
      " |          >>> x.narrow(1, 1, 2)\n",
      " |           2  3\n",
      " |           5  6\n",
      " |           8  9\n",
      " |          [torch.FloatTensor of size 3x2]\n",
      " |  \n",
      " |  ndimension(...)\n",
      " |      ndimension() -> int\n",
      " |      \n",
      " |      Alias for :meth:`~Tensor.dim()`\n",
      " |  \n",
      " |  ne(...)\n",
      " |      ne(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ne`\n",
      " |  \n",
      " |  ne_(...)\n",
      " |      ne_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.ne`\n",
      " |  \n",
      " |  nelement(...)\n",
      " |      nelement() -> int\n",
      " |      \n",
      " |      Alias for :meth:`~Tensor.numel`\n",
      " |  \n",
      " |  nonzero(...)\n",
      " |      nonzero() -> LongTensor\n",
      " |      \n",
      " |      See :func:`torch.nonzero`\n",
      " |  \n",
      " |  numel(...)\n",
      " |      numel() -> int\n",
      " |      \n",
      " |      See :func:`torch.numel`\n",
      " |  \n",
      " |  numpy(...)\n",
      " |      numpy() -> ndarray\n",
      " |      \n",
      " |      Returns this tensor as a NumPy :class:`ndarray`. This tensor and the returned\n",
      " |      :class:`ndarray` share the same underlying storage. Changes to this tensor will\n",
      " |      be reflected in the :class:`ndarray` and vice versa.\n",
      " |  \n",
      " |  prod(...)\n",
      " |      prod() -> float\n",
      " |      \n",
      " |      See :func:`torch.prod`\n",
      " |  \n",
      " |  random_(...)\n",
      " |      random_(from=0, to=None, *, generator=None)\n",
      " |      \n",
      " |      Fills this tensor with numbers sampled from the uniform distribution or\n",
      " |      discrete uniform distribution over [from, to - 1]. If not specified, the\n",
      " |      values are only bounded by this tensor's data type.\n",
      " |  \n",
      " |  remainder(...)\n",
      " |      remainder(divisor) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.remainder`\n",
      " |  \n",
      " |  remainder_(...)\n",
      " |      remainder_(divisor) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.remainder`\n",
      " |  \n",
      " |  resize_(...)\n",
      " |      resize_(*sizes)\n",
      " |      \n",
      " |      Resizes this tensor to the specified size. If the number of elements is\n",
      " |      larger than the current storage size, then the underlying storage is resized\n",
      " |      to fit the new number of elements. If the number of elements is smaller, the\n",
      " |      underlying storage is not changed. Existing elements are preserved but any new\n",
      " |      memory is uninitialized.\n",
      " |      \n",
      " |      Args:\n",
      " |          sizes (torch.Size or int...): The desired size\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
      " |          >>> x.resize_(2, 2)\n",
      " |          >>> x\n",
      " |           1  2\n",
      " |           3  4\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |  \n",
      " |  resize_as_(...)\n",
      " |      resize_as_(tensor)\n",
      " |      \n",
      " |      Resizes the current tensor to be the same size as the specified tensor. This is\n",
      " |      equivalent to::\n",
      " |      \n",
      " |          self.resize_(tensor.size())\n",
      " |  \n",
      " |  scatter_(...)\n",
      " |      scatter_(dim, index, src) -> Tensor\n",
      " |      \n",
      " |      Writes all values from the Tensor :attr:`src` into self at the indices specified\n",
      " |      in the :attr:`index` Tensor. The indices are specified with respect to the\n",
      " |      given dimension, dim, in the manner described in :meth:`~Tensor.gather`.\n",
      " |      \n",
      " |      Note that, as for gather, the values of index must be between `0` and\n",
      " |      `(self.size(dim) -1)` inclusive and all values in a row along the specified\n",
      " |      dimension must be unique.\n",
      " |      \n",
      " |      Args:\n",
      " |          input (Tensor): The source tensor\n",
      " |          dim (int): The axis along which to index\n",
      " |          index (LongTensor): The indices of elements to scatter\n",
      " |          src (Tensor or float): The source element(s) to scatter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.rand(2, 5)\n",
      " |          >>> x\n",
      " |      \n",
      " |           0.4319  0.6500  0.4080  0.8760  0.2355\n",
      " |           0.2609  0.4711  0.8486  0.8573  0.1029\n",
      " |          [torch.FloatTensor of size 2x5]\n",
      " |      \n",
      " |          >>> torch.zeros(3, 5).scatter_(0, torch.LongTensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)\n",
      " |      \n",
      " |           0.4319  0.4711  0.8486  0.8760  0.2355\n",
      " |           0.0000  0.6500  0.0000  0.8573  0.0000\n",
      " |           0.2609  0.0000  0.4080  0.0000  0.1029\n",
      " |          [torch.FloatTensor of size 3x5]\n",
      " |      \n",
      " |          >>> z = torch.zeros(2, 4).scatter_(1, torch.LongTensor([[2], [3]]), 1.23)\n",
      " |          >>> z\n",
      " |      \n",
      " |           0.0000  0.0000  1.2300  0.0000\n",
      " |           0.0000  0.0000  0.0000  1.2300\n",
      " |          [torch.FloatTensor of size 2x4]\n",
      " |  \n",
      " |  scatter_add_(...)\n",
      " |  \n",
      " |  select(...)\n",
      " |      select(dim, index) -> Tensor or number\n",
      " |      \n",
      " |      Slices the tensor along the selected dimension at the given index. If this\n",
      " |      tensor is one dimensional, this function returns a number. Otherwise, it\n",
      " |      returns a tensor with the given dimension removed.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): Dimension to slice\n",
      " |          index (int): Index to select\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          :meth:`select` is equivalent to slicing. For example,\n",
      " |          ``tensor.select(0, index)`` is equivalent to ``tensor[index]`` and\n",
      " |          ``tensor.select(2, index)`` is equivalent to ``tensor[:,:,index]``.\n",
      " |  \n",
      " |  set_(...)\n",
      " |      set_(source=None, storage_offset=0, size=None, stride=None)\n",
      " |      \n",
      " |      Sets the underlying storage, size, and strides. If :attr:`source` is a tensor,\n",
      " |      this tensor will share the same storage and have the same size and strides\n",
      " |      as the given tensor. Changes to elements in one tensor will be reflected in the\n",
      " |      other.\n",
      " |      \n",
      " |      If :attr:`source` is a :class:`~torch.Storage`, the method sets the underlying\n",
      " |      storage, offset, size, and stride.\n",
      " |      \n",
      " |      Args:\n",
      " |          source (Tensor or Storage): The tensor or storage to use\n",
      " |          storage_offset (int): The offset in the storage\n",
      " |          size (torch.Size): The desired size. Defaults to the size of the source.\n",
      " |          stride (tuple): The desired stride. Defaults to C-contiguous strides.\n",
      " |  \n",
      " |  sign(...)\n",
      " |      sign() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sign`\n",
      " |  \n",
      " |  sign_(...)\n",
      " |      sign_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sign`\n",
      " |  \n",
      " |  size(...)\n",
      " |      size() -> torch.Size\n",
      " |      \n",
      " |      Returns the size of the tensor. The returned value is a subclass of\n",
      " |      :class:`tuple`.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> torch.Tensor(3, 4, 5).size()\n",
      " |          torch.Size([3, 4, 5])\n",
      " |  \n",
      " |  sort(...)\n",
      " |      sort(dim=None, descending=False) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.sort`\n",
      " |  \n",
      " |  squeeze(...)\n",
      " |      squeeze(dim=None)\n",
      " |      \n",
      " |      See :func:`torch.squeeze`\n",
      " |  \n",
      " |  squeeze_(...)\n",
      " |      squeeze_(dim=None)\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.squeeze`\n",
      " |  \n",
      " |  storage(...)\n",
      " |      storage() -> torch.Storage\n",
      " |      \n",
      " |      Returns the underlying storage\n",
      " |  \n",
      " |  storage_offset(...)\n",
      " |      storage_offset() -> int\n",
      " |      \n",
      " |      Returns this tensor's offset in the underlying storage in terms of number of\n",
      " |      storage elements (not bytes).\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> x = torch.Tensor([1, 2, 3, 4, 5])\n",
      " |          >>> x.storage_offset()\n",
      " |          0\n",
      " |          >>> x[3:].storage_offset()\n",
      " |          3\n",
      " |  \n",
      " |  stride(...)\n",
      " |      stride() -> tuple\n",
      " |      \n",
      " |      Returns the stride of the tensor.\n",
      " |  \n",
      " |  sub(...)\n",
      " |      sub(value, other) -> Tensor\n",
      " |      \n",
      " |      Subtracts a scalar or tensor from this tensor. If both :attr:`value` and\n",
      " |      :attr:`other` are specified, each element of :attr:`other` is scaled by\n",
      " |      :attr:`value` before being used.\n",
      " |      \n",
      " |      When :attr:`other` is a tensor, the shape of :attr:`other` must be\n",
      " |      :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
      " |      tensor.\n",
      " |  \n",
      " |  sub_(...)\n",
      " |      sub_(x) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sub`\n",
      " |  \n",
      " |  sum(...)\n",
      " |      sum(dim=None) -> float\n",
      " |      \n",
      " |      See :func:`torch.sum`\n",
      " |  \n",
      " |  t(...)\n",
      " |      t() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.t`\n",
      " |  \n",
      " |  t_(...)\n",
      " |      t_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.t`\n",
      " |  \n",
      " |  topk(...)\n",
      " |      topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.topk`\n",
      " |  \n",
      " |  trace(...)\n",
      " |      trace() -> float\n",
      " |      \n",
      " |      See :func:`torch.trace`\n",
      " |  \n",
      " |  transpose(...)\n",
      " |      transpose(dim0, dim1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.transpose`\n",
      " |  \n",
      " |  transpose_(...)\n",
      " |      transpose_(dim0, dim1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.transpose`\n",
      " |  \n",
      " |  tril(...)\n",
      " |      tril(k=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.tril`\n",
      " |  \n",
      " |  tril_(...)\n",
      " |      tril_(k=0) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.tril`\n",
      " |  \n",
      " |  triu(...)\n",
      " |      triu(k=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.triu`\n",
      " |  \n",
      " |  triu_(...)\n",
      " |      triu_(k=0) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.triu`\n",
      " |  \n",
      " |  unfold(...)\n",
      " |      unfold(dim, size, step) -> Tensor\n",
      " |      \n",
      " |      Returns a tensor which contains all slices of size :attr:`size` in\n",
      " |      the dimension :attr:`dim`.\n",
      " |      \n",
      " |      Step between two slices is given by :attr:`step`.\n",
      " |      \n",
      " |      If `sizedim` is the original size of dimension dim, the size of dimension `dim`\n",
      " |      in the returned tensor will be `(sizedim - size) / step + 1`\n",
      " |      \n",
      " |      An additional dimension of size size is appended in the returned tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): dimension in which unfolding happens\n",
      " |          size (int): size of each slice that is unfolded\n",
      " |          step (int): the step between each slice\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.arange(1, 8)\n",
      " |          >>> x\n",
      " |      \n",
      " |           1\n",
      " |           2\n",
      " |           3\n",
      " |           4\n",
      " |           5\n",
      " |           6\n",
      " |           7\n",
      " |          [torch.FloatTensor of size 7]\n",
      " |      \n",
      " |          >>> x.unfold(0, 2, 1)\n",
      " |      \n",
      " |           1  2\n",
      " |           2  3\n",
      " |           3  4\n",
      " |           4  5\n",
      " |           5  6\n",
      " |           6  7\n",
      " |          [torch.FloatTensor of size 6x2]\n",
      " |      \n",
      " |          >>> x.unfold(0, 2, 2)\n",
      " |      \n",
      " |           1  2\n",
      " |           3  4\n",
      " |           5  6\n",
      " |          [torch.FloatTensor of size 3x2]\n",
      " |  \n",
      " |  unsqueeze(...)\n",
      " |      unsqueeze(dim)\n",
      " |      \n",
      " |      See :func:`torch.unsqueeze`\n",
      " |  \n",
      " |  unsqueeze_(...)\n",
      " |      unsqueeze_(dim)\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.unsqueeze`\n",
      " |  \n",
      " |  view(...)\n",
      " |      view(*args) -> Tensor\n",
      " |      \n",
      " |      Returns a new tensor with the same data but different size.\n",
      " |      \n",
      " |      The returned tensor shares the same data and must have the same number\n",
      " |      of elements, but may have a different size. A tensor must be\n",
      " |      :func:`contiguous` to be viewed.\n",
      " |      \n",
      " |      Args:\n",
      " |          args (torch.Size or int...): Desired size\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> x = torch.randn(4, 4)\n",
      " |          >>> x.size()\n",
      " |          torch.Size([4, 4])\n",
      " |          >>> y = x.view(16)\n",
      " |          >>> y.size()\n",
      " |          torch.Size([16])\n",
      " |          >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
      " |          >>> z.size()\n",
      " |          torch.Size([2, 8])\n",
      " |  \n",
      " |  zero_(...)\n",
      " |      zero_()\n",
      " |      \n",
      " |      Fills this tensor with zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.tensor._TensorBase:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      # TODO: add tests for operators\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __deepcopy__(self, _memo)\n",
      " |  \n",
      " |  __div__(self, other)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iadd__(self, other)\n",
      " |  \n",
      " |  __idiv__(self, other)\n",
      " |  \n",
      " |  __imul__(self, other)\n",
      " |  \n",
      " |  __invert__(self)\n",
      " |      # TODO: add native add or and xor in the libs\n",
      " |  \n",
      " |  __ipow__(self, other)\n",
      " |  \n",
      " |  __isub__(self, other)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __itruediv__ = __idiv__(self, other)\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __matmul__(self, other)\n",
      " |  \n",
      " |  __mod__(self, other)\n",
      " |  \n",
      " |  __mul__(self, other)\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __neg__(self)\n",
      " |  \n",
      " |  __nonzero__ = __bool__(self)\n",
      " |  \n",
      " |  __pow__(self, other)\n",
      " |  \n",
      " |  __radd__ = __add__(self, other)\n",
      " |      # TODO: add tests for operators\n",
      " |  \n",
      " |  __rdiv__(self, other)\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      helper for pickle\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmul__ = __mul__(self, other)\n",
      " |  \n",
      " |  __rsub__(self, other)\n",
      " |  \n",
      " |  __rtruediv__ = __rdiv__(self, other)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |  \n",
      " |  __truediv__ = __div__(self, other)\n",
      " |  \n",
      " |  byte(self)\n",
      " |      Casts this tensor to byte type\n",
      " |  \n",
      " |  char(self)\n",
      " |      Casts this tensor to char type\n",
      " |  \n",
      " |  chunk(self, n_chunks, dim=0)\n",
      " |      Splits this tensor into a tuple of tensors.\n",
      " |      \n",
      " |      See :func:`torch.chunk`.\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Returns a CPU copy of this tensor if it's not already on the CPU\n",
      " |  \n",
      " |  cuda = _cuda(self, device=None, async=False)\n",
      " |      Returns a copy of this object in CUDA memory.\n",
      " |      \n",
      " |      If this object is already in CUDA memory and on the correct device, then\n",
      " |      no copy is performed and the original object is returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int): The destination GPU id. Defaults to the current device.\n",
      " |          async (bool): If True and the source is in pinned memory, the copy will\n",
      " |                        be asynchronous with respect to the host. Otherwise, the\n",
      " |                        argument has no effect.\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts this tensor to double type\n",
      " |  \n",
      " |  expand_as(self, tensor)\n",
      " |      Expands this tensor to the size of the specified tensor.\n",
      " |      \n",
      " |      This is equivalent to::\n",
      " |      \n",
      " |          self.expand(tensor.size())\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts this tensor to float type\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts this tensor to half-precision float type\n",
      " |  \n",
      " |  int(self)\n",
      " |      Casts this tensor to int type\n",
      " |  \n",
      " |  is_pinned(self)\n",
      " |      Returns true if this tensor resides in pinned memory\n",
      " |  \n",
      " |  is_shared(self)\n",
      " |      Checks if tensor is in shared memory.\n",
      " |      \n",
      " |      This is always ``True`` for CUDA tensors.\n",
      " |  \n",
      " |  long(self)\n",
      " |      Casts this tensor to long type\n",
      " |  \n",
      " |  masked_copy_(self, *args, **kwargs)\n",
      " |  \n",
      " |  matmul(self, other)\n",
      " |      Matrix product of two tensors.\n",
      " |      \n",
      " |      See :func:`torch.matmul`.\n",
      " |  \n",
      " |  new(self, *args, **kwargs)\n",
      " |      Constructs a new tensor of the same data type.\n",
      " |  \n",
      " |  permute(self, *dims)\n",
      " |      Permute the dimensions of this tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          *dims (int...): The desired ordering of dimensions\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> x = torch.randn(2, 3, 5)\n",
      " |          >>> x.size()\n",
      " |          torch.Size([2, 3, 5])\n",
      " |          >>> x.permute(2, 0, 1).size()\n",
      " |          torch.Size([5, 2, 3])\n",
      " |  \n",
      " |  pin_memory(self)\n",
      " |      Copies the tensor to pinned memory, if it's not already pinned.\n",
      " |  \n",
      " |  repeat(self, *sizes)\n",
      " |      Repeats this tensor along the specified dimensions.\n",
      " |      \n",
      " |      Unlike :meth:`expand`, this function copies the tensor's data.\n",
      " |      \n",
      " |      Args:\n",
      " |          *sizes (torch.Size or int...): The number of times to repeat this\n",
      " |              tensor along each dimension\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> x = torch.Tensor([1, 2, 3])\n",
      " |          >>> x.repeat(4, 2)\n",
      " |           1  2  3  1  2  3\n",
      " |           1  2  3  1  2  3\n",
      " |           1  2  3  1  2  3\n",
      " |           1  2  3  1  2  3\n",
      " |          [torch.FloatTensor of size 4x6]\n",
      " |          >>> x.repeat(4, 2, 1).size()\n",
      " |          torch.Size([4, 2, 3])\n",
      " |  \n",
      " |  share_memory_(self)\n",
      " |      Moves the underlying storage to shared memory.\n",
      " |      \n",
      " |      This is a no-op if the underlying storage is already in shared memory\n",
      " |      and for CUDA tensors. Tensors in shared memory cannot be resized.\n",
      " |  \n",
      " |  short(self)\n",
      " |      Casts this tensor to short type\n",
      " |  \n",
      " |  split(self, split_size, dim=0)\n",
      " |      Splits this tensor into a tuple of tensors.\n",
      " |      \n",
      " |      See :func:`torch.split`.\n",
      " |  \n",
      " |  tolist(self)\n",
      " |      Returns a nested list represenation of this tensor.\n",
      " |  \n",
      " |  type = _type(self, new_type=None, async=False)\n",
      " |      Returns the type if `new_type` is not provided, else casts this object to\n",
      " |      the specified type.\n",
      " |      \n",
      " |      If this is already of the correct type, no copy is performed and the\n",
      " |      original object is returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_type (type or string): The desired type\n",
      " |          async (bool): If True, and the source is in pinned memory and\n",
      " |                        destination is on the GPU or vice versa, the copy is\n",
      " |                        performed asynchronously with respect to the host.\n",
      " |                        Otherwise, the argument has no effect.\n",
      " |  \n",
      " |  type_as(self, tensor)\n",
      " |      Returns this tensor cast to the type of the given tensor.\n",
      " |      \n",
      " |      This is a no-op if the tensor is already of the correct type. This is\n",
      " |      equivalent to::\n",
      " |      \n",
      " |          self.type(tensor.type())\n",
      " |      \n",
      " |      Params:\n",
      " |          tensor (Tensor): the tensor which has the desired type\n",
      " |  \n",
      " |  view_as(self, tensor)\n",
      " |      Returns this tensor viewed as the size as the specified tensor.\n",
      " |      \n",
      " |      This is equivalent to::\n",
      " |      \n",
      " |              self.view(tensor.size())\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.tensor._TensorBase:\n",
      " |  \n",
      " |  data\n",
      " |  \n",
      " |  shape\n",
      " |      Alias for .size()\n",
      " |      \n",
      " |      Returns a torch.Size object, containing the dimensions of the tensor\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.tensor._TensorBase:\n",
      " |  \n",
      " |  is_cuda = False\n",
      " |  \n",
      " |  is_sparse = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set = datasets.MNIST('./data', train = True, download = True, transform = transforms.Compose([ transforms.ToTensor() ]))\n",
    "help(test_set)\n",
    "help(torch.utils.data.dataset.Dataset)\n",
    "print(type(test_set.train_data))\n",
    "print(test_set.train_data.shape)\n",
    "help(torch.ByteTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.130660476274\n",
      "0.308107803856\n"
     ]
    }
   ],
   "source": [
    "print(test_set.train_data.numpy().mean(axis = (0, 1, 2))/255)\n",
    "print(test_set.train_data.numpy().std(axis = (0, 1, 2))/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "]) #  为了让数据分布在-1-1之间啦\n",
    "\n",
    "train_mnist_set = datasets.MNIST('./data', train = True, download = True,\n",
    "                      transform = transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_mnist_set, batch_size = 32, shuffle = True)\n",
    "\n",
    "test_mnist_set = datasets.MNIST('./data', train = False, download = True,\n",
    "                      transform = transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_mnist_set, batch_size = 32, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次都废了个吃屎的劲儿才把loader准备好，不容易啊，自己的数据集的构造我们还没学呢，那个需要学会自己准备存储和读取数据\n",
    "\n",
    "好，接着我们已经比较熟悉的，准备网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__() # 这里的傻逼用法，不是torch的问题，是python在多重继承时自己傻逼，然后让用户吃屎，才出现这么个恶心语法，不想研究的先记住吧\n",
    "        self.fc1 = nn.Linear(784, 20, bias = True)\n",
    "        self.fc2 = nn.Linear(20, 20, bias = True)\n",
    "        self.fc3 = nn.Linear(20, 10, bias = True) # logits\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK，网络比较简单，一气呵成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class enumerate in module builtins:\n",
      "\n",
      "class enumerate(object)\n",
      " |  enumerate(iterable[, start]) -> iterator for index, value of iterable\n",
      " |  \n",
      " |  Return an enumerate object.  iterable must be another object that supports\n",
      " |  iteration.  The enumerate object yields pairs containing a count (from\n",
      " |  start, which defaults to zero) and a value yielded by the iterable argument.\n",
      " |  enumerate is useful for obtaining an indexed list:\n",
      " |      (0, seq[0]), (1, seq[1]), (2, seq[2]), ...\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __next__(self, /)\n",
      " |      Implement next(self).\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(enumerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Loss: 0.23, Accuracy: 93.75 %\n",
      "epoch: 1, Loss: 0.31, Accuracy: 87.50 %\n",
      "epoch: 2, Loss: 0.14, Accuracy: 93.75 %\n",
      "epoch: 3, Loss: 0.16, Accuracy: 90.62 %\n",
      "epoch: 4, Loss: 0.52, Accuracy: 87.50 %\n",
      "epoch: 5, Loss: 0.11, Accuracy: 96.88 %\n",
      "epoch: 6, Loss: 0.04, Accuracy: 100.00 %\n",
      "epoch: 7, Loss: 0.11, Accuracy: 93.75 %\n",
      "epoch: 8, Loss: 0.18, Accuracy: 93.75 %\n",
      "epoch: 9, Loss: 0.10, Accuracy: 93.75 %\n",
      "epoch: 10, Loss: 0.11, Accuracy: 96.88 %\n",
      "epoch: 11, Loss: 0.36, Accuracy: 87.50 %\n",
      "epoch: 12, Loss: 0.04, Accuracy: 96.88 %\n",
      "epoch: 13, Loss: 0.05, Accuracy: 93.75 %\n",
      "epoch: 14, Loss: 0.05, Accuracy: 100.00 %\n",
      "epoch: 15, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 16, Loss: 0.06, Accuracy: 96.88 %\n",
      "epoch: 17, Loss: 0.04, Accuracy: 100.00 %\n",
      "epoch: 18, Loss: 0.02, Accuracy: 100.00 %\n",
      "epoch: 19, Loss: 0.31, Accuracy: 90.62 %\n",
      "epoch: 20, Loss: 0.14, Accuracy: 93.75 %\n",
      "epoch: 21, Loss: 0.07, Accuracy: 96.88 %\n",
      "epoch: 22, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 23, Loss: 0.04, Accuracy: 100.00 %\n",
      "epoch: 24, Loss: 0.05, Accuracy: 96.88 %\n",
      "epoch: 25, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 26, Loss: 0.12, Accuracy: 93.75 %\n",
      "epoch: 27, Loss: 0.13, Accuracy: 96.88 %\n",
      "epoch: 28, Loss: 0.10, Accuracy: 96.88 %\n",
      "epoch: 29, Loss: 0.06, Accuracy: 96.88 %\n",
      "epoch: 30, Loss: 0.15, Accuracy: 93.75 %\n",
      "epoch: 31, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 32, Loss: 0.03, Accuracy: 100.00 %\n",
      "epoch: 33, Loss: 0.12, Accuracy: 93.75 %\n",
      "epoch: 34, Loss: 0.02, Accuracy: 100.00 %\n",
      "epoch: 35, Loss: 0.02, Accuracy: 100.00 %\n",
      "epoch: 36, Loss: 0.12, Accuracy: 96.88 %\n",
      "epoch: 37, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 38, Loss: 0.03, Accuracy: 100.00 %\n",
      "epoch: 39, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 40, Loss: 0.13, Accuracy: 96.88 %\n",
      "epoch: 41, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 42, Loss: 0.13, Accuracy: 96.88 %\n",
      "epoch: 43, Loss: 0.03, Accuracy: 100.00 %\n",
      "epoch: 44, Loss: 0.02, Accuracy: 100.00 %\n",
      "epoch: 45, Loss: 0.06, Accuracy: 96.88 %\n",
      "epoch: 46, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 47, Loss: 0.08, Accuracy: 96.88 %\n",
      "epoch: 48, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 49, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 50, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 51, Loss: 0.04, Accuracy: 96.88 %\n",
      "epoch: 52, Loss: 0.04, Accuracy: 96.88 %\n",
      "epoch: 53, Loss: 0.08, Accuracy: 96.88 %\n",
      "epoch: 54, Loss: 0.08, Accuracy: 96.88 %\n",
      "epoch: 55, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 56, Loss: 0.07, Accuracy: 96.88 %\n",
      "epoch: 57, Loss: 0.08, Accuracy: 96.88 %\n",
      "epoch: 58, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 59, Loss: 0.04, Accuracy: 96.88 %\n",
      "epoch: 60, Loss: 0.11, Accuracy: 96.88 %\n",
      "epoch: 61, Loss: 0.05, Accuracy: 96.88 %\n",
      "epoch: 62, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 63, Loss: 0.08, Accuracy: 93.75 %\n",
      "epoch: 64, Loss: 0.03, Accuracy: 96.88 %\n",
      "epoch: 65, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 66, Loss: 0.03, Accuracy: 100.00 %\n",
      "epoch: 67, Loss: 0.03, Accuracy: 100.00 %\n",
      "epoch: 68, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 69, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 70, Loss: 0.06, Accuracy: 96.88 %\n",
      "epoch: 71, Loss: 0.19, Accuracy: 96.88 %\n",
      "epoch: 72, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 73, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 74, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 75, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 76, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 77, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 78, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 79, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 80, Loss: 0.06, Accuracy: 96.88 %\n",
      "epoch: 81, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 82, Loss: 0.09, Accuracy: 93.75 %\n",
      "epoch: 83, Loss: 0.09, Accuracy: 96.88 %\n",
      "epoch: 84, Loss: 0.07, Accuracy: 96.88 %\n",
      "epoch: 85, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 86, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 87, Loss: 0.02, Accuracy: 100.00 %\n",
      "epoch: 88, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 89, Loss: 0.05, Accuracy: 96.88 %\n",
      "epoch: 90, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 91, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 92, Loss: 0.07, Accuracy: 96.88 %\n",
      "epoch: 93, Loss: 0.06, Accuracy: 96.88 %\n",
      "epoch: 94, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 95, Loss: 0.01, Accuracy: 100.00 %\n",
      "epoch: 96, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 97, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 98, Loss: 0.00, Accuracy: 100.00 %\n",
      "epoch: 99, Loss: 0.01, Accuracy: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "net = MLPNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01, momentum = 0.5)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader, 0): # 0的意思是无论重跑几次，都从0开始迭代，避免多次试验时不稳定的问题\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad() # 等同于net.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1) # 注意一定是dim = 1，0是batch\n",
    "    acc = (predicted == labels).sum()\n",
    "    total = labels.size(0)\n",
    "    print(\"epoch: %d, Loss: %.2f, Accuracy: %.2f %%\" % (epoch, loss.data[0], acc.data[0] * 100 / total))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.93 %\n"
     ]
    }
   ],
   "source": [
    "# 重新定义一下\n",
    "test_loader = torch.utils.data.DataLoader(test_mnist_set, batch_size = None, shuffle = False) # None就是全部\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "    outputs = net(inputs)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    total = labels.size(0)\n",
    "    right = (predicted == labels).long().sum() # 只有使用long()才能避免傻逼torch的boolTensor累积溢出问题\n",
    "    print(\"Accuracy: %.2f %%\" % (right.data[0] * 100 / total))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000001111110000000000000000\n",
      "0000001111111111111111000000\n",
      "0000001111111111111111000000\n",
      "0000000000001011110111000000\n",
      "0000000000000000001110000000\n",
      "0000000000000000001110000000\n",
      "0000000000000000011110000000\n",
      "0000000000000000111100000000\n",
      "0000000000000000111000000000\n",
      "0000000000000000111000000000\n",
      "0000000000000001110000000000\n",
      "0000000000000011110000000000\n",
      "0000000000000011100000000000\n",
      "0000000000000111100000000000\n",
      "0000000000001111000000000000\n",
      "0000000000001110000000000000\n",
      "0000000000011110000000000000\n",
      "0000000000111110000000000000\n",
      "0000000000111110000000000000\n",
      "0000000000111000000000000000\n",
      "0000000000000000000000000000\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 7 \n",
      "  7.4015   0.6492   9.5151  15.3454 -22.6956   4.3543 -35.3334  28.3885\n",
      "\n",
      "Columns 8 to 9 \n",
      "  4.3321  13.3023\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "Variable containing:\n",
      " 7\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "7\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(test_loader))\n",
    "\n",
    "t = inputs.numpy()[0]\n",
    "for i in range(t.shape[1]):\n",
    "    for j in range(t.shape[2]):\n",
    "        if t[0][i][j] > 0:\n",
    "            print(\"1\", end='')\n",
    "        else:\n",
    "            print(\"0\", end='')\n",
    "    print(\"\")\n",
    "\n",
    "inputs = Variable(torch.Tensor(t))\n",
    "outputs = net(inputs[0])\n",
    "print(outputs)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print(predicted)\n",
    "\n",
    "labels = labels[0:1]\n",
    "\n",
    "print(labels[0])\n",
    "\n",
    "print((predicted.data == labels).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
